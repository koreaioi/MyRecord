---
layout: single
title:  "jupyter notebook 변환하기!"
categories: coding
tag: [python, blog, jekyll]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


# 합성곱 신경망 MNIST로 만들기



```python
from tensorflow.keras.datasets import reuters
import numpy as np
from tensorflow import keras
from keras import models # 모델 구성
from keras import layers # 모델 구성
import numpy as np

(train_data, train_labels),(test_data, test_labels) = reuters.load_data(num_words=10000) # label을 정수형으로 사용

# 범주형 데이터 멀티 핫 인코딩
def vectorize_sequences(sequences, dimension = 10000):
    results = np.zeros((len(sequences),dimension)) # 행 = 훈련 데이터의 양(리뷰 개수), 열 = 리뷰를 10000개로 표현
    for i, sequences in enumerate(sequences):     # i는 몇번째 리뷰인지
          for j in sequences: # sequences에는 [1,14,22,16...] 이런식으로 저장되어있음
              results[i,j] = 1
    return results

# 범주형 레이블일 경우 원 핫 인코딩
def to_one_hot(labels,dimension=46):
    results = np.zeros((len(labels),dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
```

# 데이터 준비하기



데이터와 레이블 모두 범주형이다.

범주형 데이터를 벡터로 바꾸기 위해서 멀티핫 인코딩, 레이블의 리스트를 정수 텐서로 바꾸기 위해 원 핫 인코딩을 사용한다.   

즉, train data는 멀티 핫 인코딩, train_label은 원 핫 인코딩을 사용한다.






```python
# 데이터 준비
# x는 데이터, y는 레이블이다.
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = to_one_hot(train_labels)
y_test = to_one_hot(test_labels)
```

# 모델 구성



```python
model = keras.Sequential([
    layers.Dense(64, activation = "relu"),
    layers.Dense(64, activation = "relu"),
    layers.Dense(46, activation = "softmax")
])
```


```python
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics="accuracy")
```

# 모델 정의하기

1. 뉴스를 46가지의 주제로 분류하므로 마지막 출력층의 크기는 46이다.   

2. 마지막 층에서 softmax 활성화함수를 사용한다. 46개의 값을 모두 더하면 1이 되고 이는 백분율로 나타내기 쉽기 때문이다.

3. 손실함수는 categorical_crossentropy를 사용한다.   

두 확률 분포 사이의 거리를 측정한다. 모델이 출력한 확률 분포와 진짜 레이블 분포 사이의 거리를 나타내고 이 두 분포사이의 거리를 최소화 함으로써 진짜 레이블에 가깝게 출력을 내도록 훈련한다.



* sparse_categorical_crossentropy 는 레이블이 정수형일 때 사용한다.

* 이전 예제에서는 16차원의 중간층을 사용했으나, 16차원 공간은 46개의 클래스를 구분하기에 정보의 병목 지점으로 동작할 수 있어서 충분한 크기의 중간층을 사용한다.


# 훈련 검증




```python
x_val = x_train[:1000] # train_data 중에서 0~999는 검증용으로 사용
partial_x_train = x_train[1000:] # 실제로 훈련할 데이터
y_val = y_train[:1000]
pratial_y_train = y_train[1000:]
```

train_data를 바로 훈련시키지 않고   

train_data에서 1000개의 샘플을 따로 떼어 검증 세트로 사용한다.   

이는 학습이 완료된 모델을 검증하기 위해서 사용한다.   

이 검증을 토대로 과적합이 발생하는 지점을 찾아내서 새로운 모델을 훈련할 때 그 지점 직전까지의 에포크로 훈련을 해 더 좋은 모델을 구한다.


# 모델 훈련



```python
history = model.fit(partial_x_train,
                    pratial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val,y_val)
)
```

<pre>
Epoch 1/20
16/16 [==============================] - 3s 126ms/step - loss: 2.8024 - accuracy: 0.4965 - val_loss: 1.8887 - val_accuracy: 0.6150
Epoch 2/20
16/16 [==============================] - 1s 58ms/step - loss: 1.5639 - accuracy: 0.6758 - val_loss: 1.3851 - val_accuracy: 0.6950
Epoch 3/20
16/16 [==============================] - 1s 49ms/step - loss: 1.1747 - accuracy: 0.7504 - val_loss: 1.1852 - val_accuracy: 0.7480
Epoch 4/20
16/16 [==============================] - 1s 53ms/step - loss: 0.9568 - accuracy: 0.7964 - val_loss: 1.0881 - val_accuracy: 0.7570
Epoch 5/20
16/16 [==============================] - 1s 54ms/step - loss: 0.7950 - accuracy: 0.8294 - val_loss: 0.9897 - val_accuracy: 0.7860
Epoch 6/20
16/16 [==============================] - 1s 52ms/step - loss: 0.6628 - accuracy: 0.8611 - val_loss: 0.9260 - val_accuracy: 0.8090
Epoch 7/20
16/16 [==============================] - 1s 51ms/step - loss: 0.5505 - accuracy: 0.8826 - val_loss: 0.9145 - val_accuracy: 0.8090
Epoch 8/20
16/16 [==============================] - 1s 51ms/step - loss: 0.4609 - accuracy: 0.9047 - val_loss: 0.8745 - val_accuracy: 0.8240
Epoch 9/20
16/16 [==============================] - 1s 54ms/step - loss: 0.3888 - accuracy: 0.9176 - val_loss: 0.8750 - val_accuracy: 0.8170
Epoch 10/20
16/16 [==============================] - 1s 51ms/step - loss: 0.3294 - accuracy: 0.9297 - val_loss: 0.8742 - val_accuracy: 0.8240
Epoch 11/20
16/16 [==============================] - 1s 51ms/step - loss: 0.2882 - accuracy: 0.9389 - val_loss: 0.8629 - val_accuracy: 0.8250
Epoch 12/20
16/16 [==============================] - 1s 67ms/step - loss: 0.2549 - accuracy: 0.9394 - val_loss: 0.8696 - val_accuracy: 0.8170
Epoch 13/20
16/16 [==============================] - 2s 106ms/step - loss: 0.2234 - accuracy: 0.9479 - val_loss: 0.8644 - val_accuracy: 0.8290
Epoch 14/20
16/16 [==============================] - 2s 108ms/step - loss: 0.1997 - accuracy: 0.9504 - val_loss: 0.8750 - val_accuracy: 0.8200
Epoch 15/20
16/16 [==============================] - 1s 58ms/step - loss: 0.1818 - accuracy: 0.9514 - val_loss: 0.8952 - val_accuracy: 0.8250
Epoch 16/20
16/16 [==============================] - 1s 52ms/step - loss: 0.1672 - accuracy: 0.9528 - val_loss: 0.8912 - val_accuracy: 0.8200
Epoch 17/20
16/16 [==============================] - 1s 52ms/step - loss: 0.1568 - accuracy: 0.9551 - val_loss: 0.9169 - val_accuracy: 0.8130
Epoch 18/20
16/16 [==============================] - 1s 50ms/step - loss: 0.1479 - accuracy: 0.9545 - val_loss: 0.9384 - val_accuracy: 0.8170
Epoch 19/20
16/16 [==============================] - 1s 52ms/step - loss: 0.1427 - accuracy: 0.9551 - val_loss: 0.9448 - val_accuracy: 0.8140
Epoch 20/20
16/16 [==============================] - 1s 49ms/step - loss: 0.1338 - accuracy: 0.9564 - val_loss: 0.9333 - val_accuracy: 0.8270
</pre>
1. 실질적으로 훈련되는 데이터는 parital_x_train이고 사용되는 레이블은 partial_y_train이다.



2. 훈련 횟수는 20번이고 한번에 많은 양의 데이터를 학습할 수 없으므로 배치 사이즈를 512번씩 나눠서 학습을 진행한다.



3. 검증 데이터로는 x_val, y_val 을 사용한다.


# 훈련과 검증 손실 그리기



```python
import matplotlib.pyplot as plt

loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1,len(loss)+1)
plt.plot(epochs, loss ,"bo",label="Training Loss")
plt.plot(epochs, val_loss ,"b",label="Validation Loss")
plt.title('Training and Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

<pre>
<Figure size 640x480 with 1 Axes>
</pre>
9 번째 에포크 이후부터 훈련 손실은 쭉 감소하지만 검증시 오류는 증가한다.   

이는 모델이 훈련 데이터에 과적합되어가고 있다는 뜻이다. (정확도 그래프는 반대)



그러므로 다른 특성은 그대로 두고 에포크만 수정을 할 때, 이 모델은 9 번째 에포크까지만 학습을 진행하는 것이 가장 정확도가 높은 모델이 된다는 뜻이다.


# 모델 다시 훈련하기

최적의 에포크를 찾은 모델은 실제 데이터로 학습한다.



```python
model = keras.Sequential([
    layers.Dense(64, activation = "relu"),
    layers.Dense(64, activation = "relu"),
    layers.Dense(46, activation = "softmax")
])

model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics="accuracy")

history = model.fit(x_train,
                    y_train,
                    epochs=9, # 9 번쨰까지!
                    batch_size=512,
                    validation_data=(x_val,y_val)
)

result = model.evaluate(x_test, y_test)
```

<pre>
Epoch 1/9
18/18 [==============================] - 3s 124ms/step - loss: 2.6302 - accuracy: 0.5100 - val_loss: 1.6844 - val_accuracy: 0.6570
Epoch 2/9
18/18 [==============================] - 1s 82ms/step - loss: 1.4553 - accuracy: 0.6937 - val_loss: 1.1812 - val_accuracy: 0.7490
Epoch 3/9
18/18 [==============================] - 1s 62ms/step - loss: 1.1158 - accuracy: 0.7582 - val_loss: 0.9274 - val_accuracy: 0.8070
Epoch 4/9
18/18 [==============================] - 1s 73ms/step - loss: 0.9091 - accuracy: 0.8057 - val_loss: 0.7484 - val_accuracy: 0.8510
Epoch 5/9
18/18 [==============================] - 1s 76ms/step - loss: 0.7507 - accuracy: 0.8387 - val_loss: 0.6074 - val_accuracy: 0.8760
Epoch 6/9
18/18 [==============================] - 2s 109ms/step - loss: 0.6205 - accuracy: 0.8705 - val_loss: 0.4972 - val_accuracy: 0.8980
Epoch 7/9
18/18 [==============================] - 1s 64ms/step - loss: 0.5138 - accuracy: 0.8906 - val_loss: 0.4039 - val_accuracy: 0.9340
Epoch 8/9
18/18 [==============================] - 1s 52ms/step - loss: 0.4298 - accuracy: 0.9104 - val_loss: 0.3420 - val_accuracy: 0.9310
Epoch 9/9
18/18 [==============================] - 1s 46ms/step - loss: 0.3643 - accuracy: 0.9197 - val_loss: 0.2847 - val_accuracy: 0.9470
71/71 [==============================] - 0s 4ms/step - loss: 0.9239 - accuracy: 0.7898
</pre>

```python
print(result)
```

<pre>
[0.9239057302474976, 0.7898486256599426]
</pre>
80%에 가까운 정확도를 달성했다.



만약 랜덤한 분류기를 사용했을 때 정확도는 얼마나 나올까?



랜덤한 분류기 정확도 = 0.186555....



랜덤한 분류기는 19%의 정확도가 나오므로 80%의 정확도가 나온 해당 모델은 꽤 괜찮은 모델이다.



## 랜덤한 분류기의 정확도가 19%인 이유

랜덤한 분류기라면 1/46 확률의 정확도가 나와야하는데 왜 19%가 나올까?   

이유는 46개의 클래스마다 들어있는 데이터의 양이 다르기 때문이다.

