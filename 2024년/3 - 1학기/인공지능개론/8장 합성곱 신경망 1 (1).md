# 합성곱 신경망 MNIST로 만들기



```python
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(28,28,1))
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x) # 2x2 mask로 압축한다. 입력이미지의 크기는 절반으로 줄어든다.
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.Flatten()(x)

outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)

```

## Conv2D(filters=32, kernel_size=3, activation="relu")(inputs)

filters=32: 이미지가 랜덤한 32개의 mask를거쳐 필터링된다.   

kernel_size=3: 필터링하는 mask가 3x3배열이다.   

activation="relu": 활성화함수는 relu를사용한다.   



첫번째 Conv2D가 실행되면 32개의 채널을 가진 26x26 크기의 특성맵(Response Map)이 생성된다.



## layers.MaxPooling2D(pool_size=2)(x)

2x2(pool_size) 크기의 합성곱 커널을 적용하는데 2x2 범위 내에서 최댓값을 추출하도록한다.



## layers.Flatten()(x)

입력 값 x는 3D텐서이므로 이를 Densly Connected Network에 주입하기 위해서 1D텐서로 변환한다.



## layers.Dense(10, activation="softmax")(x)

10개의 클래스(0~9)를 분류하기 위해 마지막 층의 출력 크기를 10으로하고 활성화함수를 softMax로 사용한다.



```python
model.summary()
```

<pre>
Model: "model_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 28, 28, 1)]       0         
                                                                 
 conv2d_9 (Conv2D)           (None, 26, 26, 32)        320       
                                                                 
 max_pooling2d_6 (MaxPoolin  (None, 13, 13, 32)        0         
 g2D)                                                            
                                                                 
 conv2d_10 (Conv2D)          (None, 11, 11, 64)        18496     
                                                                 
 max_pooling2d_7 (MaxPoolin  (None, 5, 5, 64)          0         
 g2D)                                                            
                                                                 
 conv2d_11 (Conv2D)          (None, 3, 3, 128)         73856     
                                                                 
 flatten_2 (Flatten)         (None, 1152)              0         
                                                                 
 dense_2 (Dense)             (None, 10)                11530     
                                                                 
=================================================================
Total params: 104202 (407.04 KB)
Trainable params: 104202 (407.04 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>
Conv2D 함수는 기본적으로 출력 특성맵이 크기가 2씩 줄어든다.   

입력과 동일한 높이와 너비를 가진 출력 특성맵을 얻고싶다면 패딩(padding)을 사용해 0으로 채워줄 수 있다.   

defalut는 valid로써 채워지지 않고 same을 사용해야한다.



```python

x = layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')



```


# 모델 훈련시키기

MNIST 이미지에서 컨브넷 훈련하기



```python
from tensorflow.keras.datasets import mnist

(train_images, train_labels),(test_images, test_labels) = mnist.load_data() # label을 정수형으로 사용
train_images = train_images.reshape((60000, 28,28,1)) # 학습셋을 28x28 크기와 60000개로 reshape
train_images = train_images.astype("float32")/255 # 0~255의 값을 0~1로 변환
test_images = test_images.reshape((10000, 28,28,1))
test_images = test_images.astype("float32") / 255
# 데이터 설정 완료

model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=['accuracy'])
model.fit(train_images, train_labels,epochs=5,batch_size=64)

# 이미지의 색이 중요한게 아니라 이미지의 특징이 중요하므로 컬러가 아닌 흑백으로 지정 (1)
# label이 원 핫 인코딩이 아닌 정수형이므로 sparse_categorical_crossentropy를 손실함수로 사용한다.
```

<pre>
Epoch 1/5
938/938 [==============================] - 51s 54ms/step - loss: 0.0423 - accuracy: 0.9869
Epoch 2/5
938/938 [==============================] - 47s 50ms/step - loss: 0.0280 - accuracy: 0.9912
Epoch 3/5
938/938 [==============================] - 45s 48ms/step - loss: 0.0202 - accuracy: 0.9941
Epoch 4/5
938/938 [==============================] - 47s 50ms/step - loss: 0.0162 - accuracy: 0.9949
Epoch 5/5
938/938 [==============================] - 46s 49ms/step - loss: 0.0125 - accuracy: 0.9963
</pre>
<pre>
<keras.src.callbacks.History at 0x7f593955fd90>
</pre>

```python
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"테스트 정확도: {test_acc:.3f}")
```

<pre>
313/313 [==============================] - 4s 12ms/step - loss: 0.0242 - accuracy: 0.9928
테스트 정확도: 0.993
</pre>
# 완전 연결층과 합성곱 층 사이의 차이



Dense 층은 입력 특성 공간에 있는 전역 패턴을 학습한다.   

합성곱 층은 지역 패턴(특징)을 학습한다.



* 합성곱 사용시 특징 2가지

1. 학습된 패턴은 평행 이동 불변성을 가진다. 즉 합성곱 층을 사용해 어떤 패턴, 특징을 학습했다면 다른 곳에서도 이 패턴을 인식할 수 있다.

2. 합성곱 층은 패턴 or 특징의 공간적 계층 구조를 학습할 수 있다.


# Maxpooling을 사용하는 이유

Conv2D를 사용하면 입력 맵의 크기가 2씩 줄어들고 MaxPooling2D를 사용하면 크기가 절반으로 줄어든다. 그럼에도 사용하는 이유는 뭘까?



다운 샘플링을 하지 않을 경우 처리할 특성 맵의 가중치 파라미터가 너무 많아져서 Overfitting이 발생한다.



# 최대 풀링 연산을 사용하는 이유

압축하고자 하는 Pooling 영역 내에서 가장 두드러진 특징을 유지하고 공간 변환 및 왜곡을 줄이기 위해서 영역 내의 최대값을 추출한다.

